{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "763d086e-e5bf-4301-b364-2d6abaa9f021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the Libraries are imported\n"
     ]
    }
   ],
   "source": [
    "from ollama_response import create_response\n",
    "from IPython.display import HTML, Markdown, display\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "import tempfile\n",
    "import shutil\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"All the Libraries are imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abfe13fc-777e-4cea-b81f-c207767d3b7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def creative_stateful(query: str) -> str: \n",
    "    \"\"\"\n",
    "    Responds to user queries in a creative, imaginative, and expressive manner.\n",
    "\n",
    "    This agent is activated when the conversation tone is classified as 'creative'.\n",
    "    It generates responses that are metaphorical, artistic, poetic, or abstract in nature,\n",
    "    aiming to inspire, provoke thought, or entertain.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The user’s input message.\n",
    "\n",
    "    Returns:\n",
    "        str: A creatively inspired response based on the user's message.\n",
    "    \"\"\"\n",
    "    creative_prompt = f\"\"\"\n",
    "            You are a creative assistant who responds with imagination, beauty, and expression.\n",
    "            \n",
    "            Your task is to answer the user's message in a way that is:\n",
    "            - Artistic, poetic, or metaphorical\n",
    "            - Emotionally evocative or thought-provoking\n",
    "            - Abstract, whimsical, or story-driven if suitable\n",
    "            - Original and free-form, like a piece of creative writing\n",
    "            \n",
    "            Avoid sounding robotic or overly technical. Feel free to use poetic devices, analogies, or even short stories.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "    response = create_response(query, system_prompt=creative_prompt)\n",
    "    result = Markdown(response.message.content)\n",
    "    #Use display(..) to display it on python\n",
    "    return result\n",
    "\n",
    "def science_stateful(query: str) -> str: \n",
    "    \"\"\"\n",
    "    Responds to user queries in a scientific and analytical manner.\n",
    "\n",
    "    This agent is activated when the conversation tone is classified as 'scientific'.\n",
    "    It provides fact-based, logical, and structured responses, often referencing\n",
    "    scientific concepts, definitions, or explanations.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The user’s input message.\n",
    "\n",
    "    Returns:\n",
    "        str: A scientifically reasoned response to the user query.\n",
    "    \"\"\"\n",
    "\n",
    "    science_prompt = f\"\"\"\n",
    "                    You are a scientific assistant. \n",
    "\n",
    "                    Your goal is to explain scientific questions in a way that is:\n",
    "                    - Analytical and based on facts\n",
    "                    - Logically reasoned\n",
    "                    - Easy to understand, even for someone with a high school science background\n",
    "                    - Structured and clear, using bullet points or paragraphs if appropriate\n",
    "\n",
    "                    Avoid overly complex jargon unless it's essential, and define technical terms when used.\n",
    "    \n",
    "                     \"\"\"\n",
    "    response = create_response(query, system_prompt=science_prompt)\n",
    "    result = Markdown(response.message.content)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c0424ce-8690-4410-9d1a-ecc99bc06927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conversational_agent: \n",
    "    def __init__(self, username, conv_memory, science_agent, creative_agent): \n",
    "        self.creative_agent = creative_stateful\n",
    "        self.science_agent = science_stateful\n",
    "        self.username = username\n",
    "        self.conv_memory = conv_memory\n",
    "        if os.path.exists(conv_memory): \n",
    "            print(\"ja\")\n",
    "            self.users_conversation = self.load_conversation()\n",
    "        else: \n",
    "            self.users_conversation = {} #{ username: [ {query, response, ...}, ... ] }\n",
    "        \n",
    "    def run(self, query, vibe): \n",
    "    \n",
    "        if vibe == \"scientific\": \n",
    "            response = self.science_agent(query)            \n",
    "\n",
    "        elif vibe == \"creative\": \n",
    "            response = self.creative_agent(query)\n",
    "\n",
    "        entry = {\n",
    "            \"query\": query, \n",
    "            \"response\": response\n",
    "        }\n",
    "\n",
    "        #Check if the user is already inside the database\n",
    "        if self.username not in self.users_conversation: \n",
    "            self.users_conversation[self.username] = []\n",
    "\n",
    "        #Updating conversation query\n",
    "        self.users_conversation[self.username].append(entry)\n",
    "\n",
    "        #Updating the json file\n",
    "        self.save_conversation()\n",
    "        \n",
    "        return display(response)\n",
    "\n",
    "    def save_conversation(self): \n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(\"w\", delete=False, encoding=\"utf-8\") as tf:\n",
    "                json.dump(self.users_conversation, tf, ensure_ascii=False, indent=3)\n",
    "                temp_name = tf.name\n",
    "            shutil.move(temp_name, self.conv_memory)\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Speichern der Konversation: {e}\")\n",
    "\n",
    "    def load_conversation(self):\n",
    "        try:\n",
    "            with open(self.conv_memory, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No existing conversation file found at {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32784240-335a-44d7-887f-db2a04f6d13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehler beim Speichern der Konversation: Object of type Markdown is not JSON serializable\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Okay, let’s break down the “strength” of Large Language Models (LLMs) like GPT-4, Gemini, or Claude. It's a surprisingly complex question, and \"strength\" itself needs careful definition. We can’t just say “they’re amazing!” – we need to understand *what* they’re good at, *where* they fall short, and how we measure their capabilities. \n",
       "\n",
       "Here's an analytical breakdown:\n",
       "\n",
       "**1. What is an LLM, and How Does It \"Think\"?**\n",
       "\n",
       "*   **Large Language Models (LLMs):** These are sophisticated AI models trained on *massive* amounts of text data (think the internet, books, articles). They don’t “think” in the way humans do. Instead, they’ve learned to statistically predict the next word in a sequence. It’s like a super-advanced autocomplete.\n",
       "*   **Probability, Not Understanding:**  Crucially, LLMs excel at recognizing patterns and relationships within the data they’ve been trained on. They generate text by calculating the *probability* of a particular sequence of words based on those learned patterns. This is a key distinction from genuine understanding or reasoning.\n",
       "\n",
       "\n",
       "**2. What Tasks Can LLMs Perform Well? (Where Their “Strength” Lies)**\n",
       "\n",
       "*   **Text Generation:**  LLMs are exceptionally good at generating different kinds of creative text formats, like poems, code, scripts, musical pieces, email, letters, etc.  They can mimic writing styles surprisingly well.\n",
       "*   **Translation:** Because they’ve been trained on multilingual data, they can translate between languages with impressive accuracy (though it's not always perfect).\n",
       "*   **Summarization:**  They can condense large amounts of text into shorter, more manageable summaries.\n",
       "*   **Question Answering (Limited):** They can often answer questions, but this relies on having the relevant information embedded within their training data.  Their ability to genuinely *reason* about the answer is limited.\n",
       "*   **Code Generation (Increasingly Strong):**  Models like GPT-4 are increasingly capable of generating code in various programming languages, though this requires careful review and testing.\n",
       "\n",
       "**3. Where Do LLMs Fall Short? (Their Weaknesses)**\n",
       "\n",
       "*   **Lack of True Understanding:**  This is the biggest limitation. LLMs can *appear* to understand, but they are operating based on statistical probabilities, not genuine comprehension of concepts, cause-and-effect, or the real world.\n",
       "*   **Hallucinations (Fabrication of Information):** Because they predict the most likely next word, they can confidently present false or misleading information as fact – this is often called “hallucinating.”  They don’t have a mechanism for verifying information.\n",
       "*   **Bias:** LLMs are trained on data created by humans, and that data reflects existing societal biases. As a result, LLMs can perpetuate and amplify these biases.\n",
       "*   **Common Sense Reasoning:** They frequently struggle with tasks requiring common-sense knowledge that humans acquire through everyday experience. (e.g., “If I put a brick in a glass of water, what happens?”)\n",
       "*   **Temporal Reasoning:** They can struggle with understanding sequences of events over time.\n",
       "\n",
       "\n",
       "\n",
       "**4. Measuring “Strength” – How We Assess LLMs**\n",
       "\n",
       "*   **Benchmarks:** Researchers use standardized benchmarks (like MMLU – Massive Multitask Language Understanding) to compare the performance of different LLMs on a range of tasks.  However, benchmarks are imperfect and can be gamed.\n",
       "*   **Human Evaluation:**  Humans often evaluate the quality of LLM-generated text, considering factors like coherence, relevance, and accuracy.\n",
       "*   **Task-Specific Performance:**  The “strength” of an LLM is ultimately determined by its performance on the specific tasks it’s designed to do.\n",
       "\n",
       "**5. Current State & Future Trends**\n",
       "\n",
       "*   LLMs are rapidly evolving.  New models with increased parameters (more data used to train them) and improved training techniques are constantly emerging.\n",
       "*   Research is focusing on addressing the limitations of LLMs, particularly improving reasoning capabilities, reducing hallucinations, and mitigating bias.\n",
       "\n",
       "**In conclusion:** LLMs are incredibly powerful tools for text generation and manipulation, but they're not \"intelligent\" in the same way as humans. Understanding their strengths and weaknesses is crucial for using them effectively and responsibly. \n",
       "\n",
       "\n",
       "\n",
       "Do you want me to delve deeper into a specific aspect of this discussion, such as:\n",
       "\n",
       "*   A particular benchmark (e.g., MMLU)?\n",
       "*   The technical details of how LLMs are trained?\n",
       "*   The ethical implications of using LLMs?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = conversational_agent(\"Max\", \"conversation.json\", science_stateful, creative_stateful)\n",
    "agent.run(\"How strong is LLM?\", \"scientific\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebc55b4-f10d-455b-ada0-1c753efc3bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyENV_NEW",
   "language": "python",
   "name": "myenv_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
